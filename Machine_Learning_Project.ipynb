{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cryptocurrency Liquidity Prediction for Market Stability\n",
        "\n",
        "1. Exploratory Data Analysis (EDA) Report\n",
        "\n",
        "- the EDA Report summarizes the dataset statistics and provides basic visualizations of trends, correlations, and distributions.\n",
        "  1. Introduction:\tBriefly state the goal of the EDA: to understand the structure, quality, and initial patterns in the cryptocurrency dataset (e.g., coin_gecko_2022-03-17.csv and coin_gecko_2022-03-16.csv)\n",
        "  2. Dataset Overview\tShow the first few rows of the data, list the features (coin, price, 24h_volume, mkt_cap, etc.), and provide data types and unique value counts.\n",
        "  3. Data Quality Assessment\n",
        "    - Missing Values: Identify and quantify any missing values in columns like price or 24h_volume.\n",
        "    - Data Consistency: Note any inconsistencies .\n",
        "    - Outlier Detection: Identify extreme outliers in volume or price data.\n",
        "  4. Univariate Analysis\n",
        "    - Visualize the distribution of key numerical variables (e.g., price, 24h_volume, mkt_cap) using histograms.\n",
        "    - Analyze the distribution of the target variable (liquidity proxy, possibly a function of 24h_volume/mkt_cap).\n",
        "  5. Multivariate Analysis\n",
        "    - Generate a correlation matrix to find relationships between variables.\n",
        "    - Plot time series trends for the top N coins (e.g., Bitcoin, Ethereum) showing price vs. date.\n",
        "    - Visualize the correlation between a coin's price change (e.g., 24h change) and its 24h_volume\n",
        "  6. Conclusion\tSummarize the key findings and how they will inform the subsequent Feature Engineering and Model Selection steps.\n",
        "\n",
        "2. HLD & LLD Document\n",
        " - This document should include a High-Level Design that provides an overview of the system and architecture, and a Low-Level Design that details how each component is implemented.\n",
        "\n",
        " - High-Level Design: The HLD focuses on the overall system structure, major components, and how they interact.\n",
        " - System Components: Define the major blocks: Data Ingestion , Data Storage, Data Processing/ML Training (Jupyter/Python scripts), ML Model, and Deployment/Prediction API .\n",
        " - Architecture Diagram: Sketch the flow of data from the source to the final prediction.\n",
        " - Technology Stack: List the main technologies: Python, Pandas, Scikit-learn/TensorFlow/PyTorch, and Flask/Streamlit .\n",
        "\n",
        "3. Pipeline Architecture and Document\n",
        " - This deliverable specifically focuses on the data flow for the project. This section is closely related to the HLD but specifically diagrams and explains the automated workflow.\n",
        " - Diagram: Provide a clear diagram showing the flow.\n",
        " - Stages of the Pipeline:\n",
        "    - Data Acquisition: Raw data is collected from the source.\n",
        "    - Data Validation/Cleaning: Missing values are handled, and data types are corrected.\n",
        "    - Feature Engineering: New features, including the target liquidity metric, are created.\n",
        "    - Model Training & Evaluation: The processed data is used to train the ML model, and performance is evaluated.\n",
        "    - Model Register/Deployment: The best performing model is saved and made available for inference.\n",
        "    - Inference/Prediction: The deployed model receives new data and returns a liquidity prediction.\n",
        "\n",
        "4. Final Report:\n",
        " - The Final Report summary of the project's findings, model performance, and key insights.\n",
        "   1. Executive Summary\tA concise, non-technical summary of the project goal, methodology, and main result\n",
        "   2. Problem Statement & Objective\tReiterate the need for a model to predict  cryptocurrency liquidity for market stability and risk management.\n",
        "   3. Methodology\n",
        "    - Data Preprocessing: Summarize the key steps taken.\n",
        "    -  Model Selection: Justify the final model chosen and the features that had the most impact.\n",
        "    4. Results & Evaluation\tPresent the model's performance on the test data using the agreed-upon metrics. Discuss the robustness of the model.\n",
        "    5. Key Insights & Findings\n",
        "    - Liquidity Drivers: What market factors (e.g., 24h_volume, 7d price change, new features) were the strongest predictors of liquidity?\n",
        "    - Market Stability: What do the model's predictions suggest about the future stability of the cryptocurrency market.\n",
        "    6. Conclusion & Future Work: Conclude by summarizing the project's success. Suggest next steps, such as deploying the model for real-time predictions or incorporating new data sources"
      ],
      "metadata": {
        "id": "egkKQPzdg7so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# python crypto_liquidity_project.py\n",
        "\n",
        "'''\n",
        "'''\n",
        "# crypto_liquidity_project.py\n",
        "# Complete pipeline for Cryptocurrency Liquidity Prediction\n",
        "# Requirements: pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost, joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Load & combine CSV files\n",
        "# ------------------------------\n",
        "file1 = '/mnt/data/coin_gecko_2022-03-16.csv'\n",
        "file2 = '/mnt/data/coin_gecko_2022-03-17.csv'\n",
        "\n",
        "if not (os.path.exists(file1) and os.path.exists(file2)):\n",
        "    raise FileNotFoundError(f\"Make sure both CSV files exist at {file1} and {file2}\")\n",
        "\n",
        "df = pd.concat([pd.read_csv(file1), pd.read_csv(file2)], ignore_index=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Combined dataframe shape:\", df.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Preprocessing & Feature Engineering\n",
        "# ------------------------------\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# engineered features\n",
        "df['liquidity_ratio'] = df['24h_volume'] / df['mkt_cap']\n",
        "df['volatility_index'] = df[['1h','24h','7d']].std(axis=1)\n",
        "df['price_stability_score'] = 1 - (df['24h'].abs() / (df['7d'].replace(0, np.nan).abs()+1e-9))\n",
        "\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "numeric_cols = ['price','24h_volume','mkt_cap','liquidity_ratio','volatility_index','price_stability_score']\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "print(\"After preprocessing shape:\", df.shape)\n",
        "print(df[numeric_cols].describe().T)\n",
        "\n",
        "# Save a short CSV of processed data for quick inspection\n",
        "os.makedirs('/mnt/data/processed', exist_ok=True)\n",
        "df.to_csv('/mnt/data/processed/crypto_processed.csv', index=False)\n",
        "print(\"Saved processed CSV to /mnt/data/processed/crypto_processed.csv\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Exploratory Data Analysis (plots will be saved)\n",
        "# ------------------------------\n",
        "os.makedirs('/mnt/data/plots', exist_ok=True)\n",
        "\n",
        "# Distribution of price\n",
        "plt.figure()\n",
        "sns.histplot(df['price'], bins=40, kde=True)\n",
        "plt.title('Distribution of Cryptocurrency Prices (Normalized)')\n",
        "plt.xlabel('Normalized Price')\n",
        "plt.savefig('/mnt/data/plots/price_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "corr = df[['price','24h_volume','mkt_cap','liquidity_ratio','volatility_index','price_stability_score']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.savefig('/mnt/data/plots/correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Liquidity Ratio vs Market Cap scatter\n",
        "plt.figure()\n",
        "sns.scatterplot(x='mkt_cap', y='liquidity_ratio', data=df, alpha=0.7)\n",
        "plt.title('Liquidity Ratio vs Market Cap')\n",
        "plt.savefig('/mnt/data/plots/liquidity_vs_mktcap.png')\n",
        "plt.close()\n",
        "\n",
        "# Volatility vs Stability\n",
        "plt.figure()\n",
        "sns.scatterplot(x='volatility_index', y='price_stability_score', data=df, alpha=0.7)\n",
        "plt.title('Volatility vs Price Stability')\n",
        "plt.savefig('/mnt/data/plots/volatility_vs_stability.png')\n",
        "plt.close()\n",
        "\n",
        "# Top 10 by liquidity ratio\n",
        "top10 = df.sort_values('liquidity_ratio', ascending=False).head(10)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x='liquidity_ratio', y='coin', data=top10)\n",
        "plt.title('Top 10 Cryptocurrencies by Liquidity Ratio')\n",
        "plt.savefig('/mnt/data/plots/top10_liquidity.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved EDA plots to /mnt/data/plots\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Modeling: Train & Evaluate\n",
        "# ------------------------------\n",
        "feature_cols = ['price','24h','7d','24h_volume','mkt_cap','volatility_index','price_stability_score']\n",
        "X = df[feature_cols]\n",
        "y = df['liquidity_ratio']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# 1) Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# 2) Random Forest (small grid)\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_params = {'n_estimators':[50,100], 'max_depth':[6,10]}\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "rf_best = rf_grid.best_estimator_\n",
        "y_pred_rf = rf_best.predict(X_test)\n",
        "\n",
        "# 3) XGBoost (small grid)\n",
        "xgb = XGBRegressor(random_state=42, n_jobs=1, verbosity=0)\n",
        "xgb_params = {'n_estimators':[50,100], 'max_depth':[3,5], 'learning_rate':[0.05,0.1]}\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "xgb_best = xgb_grid.best_estimator_\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    return {\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'R2': r2_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "results = {\n",
        "    'Linear Regression': evaluate(y_test, y_pred_lr),\n",
        "    'Random Forest': evaluate(y_test, y_pred_rf),\n",
        "    'XGBoost': evaluate(y_test, y_pred_xgb)\n",
        "}\n",
        "\n",
        "print(\"Evaluation results:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(metrics)\n",
        "\n",
        "# Save models\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "joblib.dump(lr, '/mnt/data/models/linear_regression.joblib')\n",
        "joblib.dump(rf_best, '/mnt/data/models/random_forest.joblib')\n",
        "joblib.dump(xgb_best, '/mnt/data/models/xgboost.joblib')\n",
        "joblib.dump(scaler, '/mnt/data/models/scaler.joblib')\n",
        "print(\"Saved models to /mnt/data/models\")\n",
        "\n",
        "# End of script\n",
        "print(\"Pipeline complete. Check /mnt/data for outputs (processed CSV, plots, and models).\")\n",
        "\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "# Complete notebook: load data, preprocess, EDA, feature engineering, models (Linear Regression, Random Forest, XGBoost), evaluation, and save models.\n",
        "\n",
        "'''\n",
        "'''\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "# Imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ense mble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "file1 = '/mnt/data/coin_gecko_2022-03-16.csv'\n",
        "file2 = '/mnt/data/coin_gecko_2022-03-17.csv'\n",
        "\n",
        "df = pd.concat([pd.read_csv(file1), pd.read_csv(file2)], ignore_index=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print('Combined shape:', df.shape)\n",
        "df.head()\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "df['liquidity_ratio'] = df['24h_volume'] / df['mkt_cap']\n",
        "df['volatility_index'] = df[['1h','24h','7d']].std(axis=1)\n",
        "df['price_stability_score'] = 1 - (df['24h'].abs() / (df['7d'].replace(0, np.nan).abs()+1e-9))\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "numeric_cols = ['price','24h_volume','mkt_cap','liquidity_ratio','volatility_index','price_stability_score']\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "df[numeric_cols].describe().T\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "# distribution\n",
        "sns.histplot(df['price'], bins=40, kde=True)\n",
        "plt.title('Distribution of Price (Normalized)')\n",
        "plt.show()\n",
        "\n",
        "# correlation heatmap\n",
        "corr = df[['price','24h_volume','mkt_cap','liquidity_ratio','volatility_index','price_stability_score']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "# liquidity vs mktcap\n",
        "sns.scatterplot(x='mkt_cap', y='liquidity_ratio', data=df, alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# volatility vs stability\n",
        "sns.scatterplot(x='volatility_index', y='price_stability_score', data=df, alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# top 10 liquidity\n",
        "top10 = df.sort_values('liquidity_ratio', ascending=False).head(10)\n",
        "sns.barplot(x='liquidity_ratio', y='coin', data=top10)\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "feature_cols = ['price','24h','7d','24h_volume','mkt_cap','volatility_index','price_stability_score']\n",
        "X = df[feature_cols]\n",
        "y = df['liquidity_ratio']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Random Forest (small grid)\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_params = {'n_estimators':[50,100], 'max_depth':[6,10]}\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "rf_best = rf_grid.best_estimator_\n",
        "y_pred_rf = rf_best.predict(X_test)\n",
        "\n",
        "# XGBoost (small grid)\n",
        "xgb = XGBRegressor(random_state=42, n_jobs=1, verbosity=0)\n",
        "xgb_params = {'n_estimators':[50,100], 'max_depth':[3,5], 'learning_rate':[0.05,0.1]}\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "xgb_best = xgb_grid.best_estimator_\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "def evaluate(y_true, y_pred):\n",
        "    return {'MAE': mean_absolute_error(y_true, y_pred), 'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)), 'R2': r2_score(y_true, y_pred)}\n",
        "\n",
        "results = {\n",
        "    'Linear Regression': evaluate(y_test, y_pred_lr),\n",
        "    'Random Forest': evaluate(y_test, y_pred_rf),\n",
        "    'XGBoost': evaluate(y_test, y_pred_xgb)\n",
        "}\n",
        "results\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "joblib.dump(lr, '/mnt/data/models/linear_regression.joblib')\n",
        "joblib.dump(rf_best, '/mnt/data/models/random_forest.joblib')\n",
        "joblib.dump(xgb_best, '/mnt/data/models/xgboost.joblib')\n",
        "joblib.dump(scaler, '/mnt/data/models/scaler.joblib')\n",
        "print(\"Saved models in /mnt/data/models\")\n",
        "\n",
        "'''\n",
        "'''\n"
      ],
      "metadata": {
        "id": "WTlAp6xWlkKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IUhGBK_rk06N"
      }
    }
  ]
}